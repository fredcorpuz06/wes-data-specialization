So now let's see how to generate
a random forest with Python. Again, I'm going to use the Wave One, Add
Health Survey that I have data managed for the purpose of growing decision trees. You'll recall that there
are several variables. Again, we'll define the response or
target variable, regular smoking, based on answers to the question,
have you ever smoked cigarettes regularly? That is, at least one cigarette
every day for 30 days. The candidate explanatory variables
include gender, race, alcohol, marijuana, cocaine, or inhalant use. Availability of cigarettes in the home,
whether or not either parent was
on public assistance, any experience with being expelled
from school, age, alcohol problems, deviance, violence, depression,
self esteem, parental presence, activities with parents family and school
connectedness and grade point average. Much of the code that we'll write for
our random forest will be quite similar to the code we had written for
individual decision trees. First there are a number of
libraries that we need to call in, including features from
the sklearn library. Next I'm going to use the change working
directory function from the OS library to indicate where my data set is located. next I'll load my data set
called tree_addhealth.csv. because decision tree analyses cannot
handle any NAs in our data set, my next step is to create a clean
data frame that drops all NAs. Setting the new data frame
called data_clean I can now take a look at various characteristics of
my data, by using the D types and describe functions to examine data
types and summary statistics. Next I set my explanatory and
response, or target variables, and then include the train test split
function for predictors and target. And set the size ratio to 60% for
the training sample, and 40% for the test sample by
indicating test_size=.4. Here I request the shape of
these predictor and target and training test samples. From sklearn.ensamble I import
the RandomForestClassifier. Now that training and test data sets have
already been created, we'll initialize the random forest classifier from
SK Learn and indicate n_estimators=25. n_estimators are the number of trees
you would build with the random forest algorithm. Next I actually fit the model
with the classifier.fit function which we passed the training
predictors and training targets too. Then, we go unto the prediction
on the testator set. And we could also similar to
decision tree code as for the confusion matrix and accuracy scores. For the confusion matrix,
we see the true negatives and true positives on the diagonal. And the 207 and the 82 represent the false negatives and
false positives, respectively. Notice that the overall accuracy for
the forest is 0.84. So 84% of the individuals
were classified correctly, as regular smokers, or
not regular smokers. Given that we don't interpret
individual trees in a random forest, the most helpful information
to be gotten from a forest is arguably the measured importance for
each explanatory variable. Also called the features. Based on how many votes or splits each
has produced in the 25 tree ensemble. To generate importance scores, we
initialize the extra tree classifier, and then fit a model. Finally, we ask Python to print
the feature importance scores calculated from the forest of trees that we've grown. The variables are listed in the order
they've been named earlier in the code. Starting with gender, called BIO_SEX,
and ending with parental presence. As we can see the variables with
the highest important score at 0.13 is marijuana use. And the variable with the lowest important
score is Asian ethnicity at .006. As you will recall,
the correct classification rate for the random forest was 84%. So were 25 trees actually needed to get
this correct rate of classification? To determine what growing larger number of
trees has brought us in terms of correct classification. We're going to use code that builds for
us different numbers of trees, from one to 25, and provides the correct
classification rate for each. This code will build for us random
forest classifier from one to 25, and then finding the accuracy score for
each of those trees from one to 25, and storing it in an array. This will give me 25
different accuracy values. And we'll plot them as
the number of trees increase. As you can see, with only one
tree the accuracy is about 83%, and it climbs to only about 84% with
successive trees that are grown giving us some confidence that it may
be perfectly appropriate to interpret a single decision tree for this data. Given that it's accuracy is quite near
that of successive trees in the forest. To summarize, like decision trees,
random forests are a type of data mining algorithm that can select from
among a large number of variables. Those that are most important
in determining the target or response variable to be explained. Also light decision trees. The target variable in a random forest
can be categorical or quantitative. And the group of explanatory variables or features can be categorical and
quantitative in any combination. Unlike decision trees however, the results of random forests
often generalize well to new data. Since the strongest signals are able to
emerge through the growing of many trees. Further, small changes in the data do not
impact the results of a random forest. In my opinion, the main weakness
of random forests is simply that the results are less satisfying,
since no trees are actually interpreted. Instead, the forest of trees is used to rank the importance of variables
in predicting the target. Thus we get a sense of the most
important predictive variables but not necessarily their
relationships to one another.