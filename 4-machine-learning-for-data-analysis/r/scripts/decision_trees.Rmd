---
title: "Decision Trees"
output: html_notebook
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

Lets learn how to generate a decision tree. 

### Import libraries 

```{r}
library(tidyverse) # data manipulation
library(caret) # machine learning 
library(rpart) # decision trees
library(rpart.plot) # visualize trees
```

### Load the dataset

I'm going to point my working directory to where my data files are located with
`setwd()`. Then I'm going to view all the columns that are in the dataset. 

```{r, echo=FALSE}
setwd("C:/Users/fcorp/Desktop/repos/wes-data-specialization/data")
df <- read_csv("tree_addhealth.csv") 
colnames(df)
````

Next, I will pick out the variables that I need - everything except 
`id`. Then, make sure that `TREG1` (our dependent variable) is a factor. Lastly,
create a clean data frame that drops all NA's

```{r}
df <- df %>% 
    select(-id) %>%
    mutate(TREG1 = factor(TREG1)) %>%
    na.omit()

head(df)
```

### Split into train and test

I will split my data into a training and testing set. The size ratio will be 
60% for the training sample and 40% for the test sample, indicated by `p = 0.6` 
in `createDataPartition()`.

```{r}
set.seed(1234)
trainIndex <- createDataPartition(df$TREG1, p = 0.6, list = FALSE, times = 1)

train <- df[trainIndex, ]
test <- df[-trainIndex, ]
```

Here I request the shape of these predictor and target training and test 
samples. The training sample has `r dim(train)[1]` observations or rows, 60% of 
our original sample, and `r dim(train)[2]` explanatory variables. The test sample
has `r dim(test)[1]` observations or rows. 40% of the original sample. And again
`r dim(test)[2]` explanatory variables or columns.

```{r}
dim(train)
dim(test)

```

### Build model on train data

Once training and testing data sets have been created, we train our classifier 
with `rpart()`. We input the `train` dataset, pass the dependent variable `TREG1`
and all the rest of the variables as the predictors with `.` 

```{r}
classifier <- rpart(TREG1 ~ ., data = train, method = "class")
```

### Predict and evaluate

Next we include `predict()` where we predict `TREG1` for the values in `test` 
and save those values into `pred`. Then, we call in `confusionMatrix()` to which
we passed our prediction `pred` and the actual labels `test$TREG1`. 
```{r}
pred <- predict(classifier, newdata = test, type = "class")
confusionMatrix(pred, reference = test$TREG1)
```

```{r, include = FALSE}
cm <- confusionMatrix(pred, test$TREG1)

vals <- as.list(as.vector(cm$table))
names(vals) <- c("tneg", "fneg", "fpos", "tpos")
list2env(vals, envir = parent.frame())
ac <- round(cm$overall['Accuracy'], 2)
```
This shows the correct and 
incorrect classifications of our decision tree. The diagonal, `r tneg` and `r tpos`,
represent the number of true negative for regular smoking, and the number
of true positives, respectively. The `r fneg`, on the bottom left,
represents the number of false negatives. Classifying regular smokers
as not regular smokers. And the `r fneg` on the top right,
the number of false positives, classifying a non regular
smoker as a regular smoker. We can also look at the accuracy
score which is approximately `r ac`, which suggests that the decision
tree model has classified `r ac*100`% of the sample correctly as either
regular or not regular smokers.

###  Display and interpret decision tree

But what does our decision tree look like? 

```{r}
rpart.plot(classifier, type = 0, extra = 101)
```


```{r, include=FALSE}
pl <- rpart.plot(classifier, type = 0, extra = 101)$labs %>% na.omit() %>% as.list
pl <- lapply(pl, function(x) strsplit(x, "[^0-9]+")[[1]] %>% as.numeric)

```
Here I show a tree with `TREG1`, my binary regular smoking variable, as the 
target. The resulting tree starts with the split on `marever1`, our first 
explanatory variable on marijuana use. `marever1` is a binary variable with 
`0 = no marijuana use since birth` and `1 = has used marijuana`. If an 
observation has `marver1=0` then the observations move to the left side of the 
split -  `r pl[[1]][4]`% of the individuals in the training sample are predicted
to be NOT regular smokers, `TREG = 0`. (Of the observations in this node, there 
are `r pl[[1]][2]` observations that are correctly `TREG = 0` and `r pl[[1]][3]`
that are in the `TREG = 1` but part of this prediction group) 

From this node, another split is made on `WHITE`, such that among those 
individuals in the `marever1 = 1` group (have never smoked) in the first split 
and also belong to the `r WHITE = 0` in the second split, `r pl[[2]][4]`% of the
individuals in the training sample are predicted to be NOT regular smokers, 
`TREG = 0`. (Of the observations in this node, there 
are `r pl[[2]][2]` observations that are correctly `TREG = 0` and `r pl[[2]][3]`
that are in the `TREG = 1`  but part of this prediction group) 

The same interpretations can be made at each of the splits `DEP1 < 10`,
`DEVIANT 1 < 6`, and `cocever1 = 0`. 

### Summary 

To summarize, decision trees are a type of data-mining algorithm that can select 
from among a large number of variables those
in their interactions that are most important in determining the target or
response variable to be explained. They create segmentations or
subgroups in the data by applying a series of simple rules or
criteria over and over again, which choose variable constellations
that best predict the target variable. In this way, they allow for
a data driven exploration of non-linear relationships and
interactions among explanatory variables. As we've seen, an advantage of decision
trees is they're easy to interpret and visualize especially when
the tree is very small. Tree based methods also
handle large data-sets well. And can predict both binary,
categorical target variables, as shown in our example, and
also quantitative target variables. However, as we've also shown, small
changes in the data can lead to different splits and this can undermine
the interpretability of the model. Also decision trees are not very
reproducible on future data. This is a detail that we will return
to in our next topic random forests.