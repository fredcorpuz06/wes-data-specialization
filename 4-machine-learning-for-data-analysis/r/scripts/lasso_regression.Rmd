---
title: "Lasso Regression"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

Lets learn how to run a lasso regression. 

### Import libraries 

```{r}
library(tidyverse) # data manipulation
library(caret) # machine learning 
library(glmnet) # lasso regression
```

### Load the dataset

I'm going to point my working directory to where my data files are located with
`setwd()`. Then I'm going to view all the columns that are in the dataset. 

```{r, echo=FALSE}
setwd("C:/Users/fcorp/Desktop/repos/wes-data-specialization/data")
df <- read_csv("tree_addhealth.csv") 
colnames(df)
````

Next, I will pick out the variables that I need - everything except 
`id`. Then, make sure that `SCHCONN1` (our dependent variable) is numeric. Lastly,
create a clean data frame that drops all NA's

```{r}
df <- df %>% 
  mutate(MALE = ifelse(BIO_SEX == 1, 1, 0),
         SCHCONN1 = as.numeric(SCHCONN1)) %>%
  select(-id, -BIO_SEX) %>% 
  na.omit()

head(df)
```

### Center and scale 

In lasso regression, the penalty term is not fair if the predictive variables
are not on the same scale, meaning that not all the predictors
get the same penalty. So I will standardize all the predictors
to have a mean equal to zero and a standard deviation equal to one,
including my binary predictors, which will put them all on the same scale. 

Lets create `my_center_scale()` that transforms the variable to have a mean of 
zero and a standard deviation of one, thus putting all the predictors on the 
same scale.

```{r}
my_center_scale <- function(var, seed = 1234){
  set.seed(seed)
  centered <- var - mean(var)
  scaled <- centered / sd(var)
  
  return(scaled)
}

# Test if our function works
df$GPA1[1:20]
gpa1_scaled <- my_center_scale(df$GPA1[1:20])
gpa1_scaled
mean(gpa1_scaled)
sd(gpa1_scaled)
```

Now lets apply our function to all our predictors. 

```{r}
X <- df %>% select(-SCHCONN1)
y <- df$SCHCONN1

X <- map_df(X, my_center_scale)
head(X)
```




### Split into train and test

I will split my data into a training and testing set. The size ratio will be 
60% for the training sample and 40% for the test sample, indicated by `p = 0.6` 
in `createDataPartition()`.

```{r}
set.seed(1234)
trainIndex <- createDataPartition(df$SCHCONN1, p = 0.6, list = FALSE, times = 1)

X_train <- as.matrix(X[trainIndex, ]) # glmnet() expects a matrix input
X_test <- as.matrix(X[-trainIndex, ]) 
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
```

Here I request the shape of these predictor and target training and test 
samples. The training sample has `r dim(X_train)[1]` observations or rows, 60% of 
our original sample, and `r dim(X_train)[2]` explanatory variables. The test sample
has `r dim(X_test)[1]` observations or rows. 40% of the original sample. And again
`r dim(X_test)[2]` explanatory variables or columns.

```{r}
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```



### Build model on train data

Once training and testing data sets have been created, we train our classifier 
with `train()`. We input the `train` dataset, pass the dependent variable 
`SCHCONN1` and all the rest of the variables as the predictors with `.` 


```{r}
# alpha = 1 is lasso penalty
# glmnet() will perform lasso regression w/o cross-validation
set.seed(1234)
regressor <- cv.glmnet(X_train, y_train, family = "gaussian", alpha = 1) 
plot(regressor, xlab = "lambda", main = "Lasso")

```

```{r}
bestlam <- regressor$lambda.min
bestlam

```




### Predict and evaluate

Next we include `predict()` where we predict `SCHCONN1` for the values in `test` 
and save those values into `pred`. Then, we call in `confusionMatrix()` to which
we passed our prediction `pred` and the actual labels `test$TREG1`. 
```{r}
# predict class assuming prob cutoff of .5 
pred <- predict(regressor, newx=X_test)
rmse.lasso <- sqrt(mean((pred - y_test)^2))
rmse.lasso
```

